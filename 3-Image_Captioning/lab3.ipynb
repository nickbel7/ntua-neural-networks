{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFwSaNB8jF7s"
      },
      "source": [
        "### Στοιχεία ομάδας (Ομάδα 1):\n",
        "\n",
        "```\n",
        "Neural Networks - Lab 3\n",
        "Creators : \n",
        "Αναστάσης Αγγλογάλλος\n",
        "Παναγιώτης Κοκκινάκης\n",
        "Νικόλας Μπέλλος\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "id": "Ah8NCF9FH9hQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89a01e2d-7b31-416b-a481-573a18caf446"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.chdir(\"/content/gdrive/MyDrive/LAB-3 | Image_Captioning/\")\n",
        "\n",
        "!ls"
      ],
      "metadata": {
        "id": "agGGkcKHJal1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d23da0f2-934c-4e42-8188-9cbc4b7f7189"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " answer.txt\t\t\t\t test_cache\n",
            " BELLOS-Lab3-image_captioning.ipynb\t test_cache_features\n",
            " BELLOS-v2-Lab3-image_captioning.ipynb\t test_files.csv\n",
            " captions_new.csv\t\t\t test_test\n",
            " competition_files.csv\t\t\t train_cache\n",
            " image_dir\t\t\t\t train_cache_features\n",
            "'image_dir (1)'\t\t\t\t train_files.csv\n",
            " Lab3-image_captioning.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Confirm what GPU we are running on\n",
        "!nvidia-smi -L\n",
        "!lscpu |grep 'Model name'"
      ],
      "metadata": {
        "id": "lLH3Sd5kCkNF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1596141c-f3ad-425a-c64b-97dce20ddb5d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-c235744c-7ad6-b705-006c-b3a128665e0a)\n",
            "Model name:                      Intel(R) Xeon(R) CPU @ 2.00GHz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cffg2i257iMS"
      },
      "source": [
        "# Image captioning with visual attention"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intro"
      ],
      "metadata": {
        "id": "0Bdu5ZZ_lOUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WHAT IS THIS NOTEBOOK ?**<BR>\n",
        "This notebook is an end-to-end example. When you run the notebook, it downloads a dataset, extracts and caches the image features, and trains a decoder model. It then uses the model to generate captions on new images."
      ],
      "metadata": {
        "id": "g11J7LJrWYH3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QASbY_HGo4Lq"
      },
      "source": [
        "The model architecture used here is inspired by [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044), but has been updated to use a 2-layer Transformer-decoder. To get the most out of this tutorial you should have some experience with [text generation](https://www.tensorflow.org/text/tutorials/text_generation),  [seq2seq models & attention](https://www.tensorflow.org/text/tutorials/nmt_with_attention), or [transformers](https://www.tensorflow.org/text/tutorials/transformer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HbD8n0w7d3F"
      },
      "source": [
        "The model architecture built in this tutorial is shown below. Features are extracted from the image, and passed to the cross-attention layers of the Transformer-decoder.\n",
        "<table>\n",
        "<tr>\n",
        "  <th>The model architecture</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=250 src=\"https://tensorflow.org/images/tutorials/transformer/ImageCaptioning.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IxifZKT6vXQ"
      },
      "source": [
        "The transformer decoder is mainly built from attention layers. It uses self-attention to process the sequence being generated, and it uses cross-attention to attend to the image.\n",
        "\n",
        "By inspecting the attention weights of the cross attention layers you will see what parts of the image the model is looking at as it generates words.\n",
        "<details>\n",
        "<summary><b>Example</b></summary>\n",
        "\n",
        "![Prediction](https://tensorflow.org/images/imcap_prediction.png)\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bwwk4uxRz6A"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gc06pTaBbl72"
      },
      "outputs": [],
      "source": [
        "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2R1hQGtZEi8Y"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y tensorflow estimator keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tensorflow_text tensorflow tensorflow_datasets"
      ],
      "metadata": {
        "id": "AwY76jiAz-MP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TGZmOuqMia9"
      },
      "outputs": [],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "U8l4RJ0XRPEm"
      },
      "outputs": [],
      "source": [
        "import concurrent.futures\n",
        "import collections\n",
        "import dataclasses\n",
        "import hashlib\n",
        "import itertools\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "import urllib.request\n",
        "\n",
        "import einops\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import requests\n",
        "import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5e_SigQFiWf"
      },
      "source": [
        "### Choose a dataset\n",
        "\n",
        "This tutorial is set up to give a choice of datasets. Either [Flickr8k](https://www.ijcai.org/Proceedings/15/Papers/593.pdf) or a small slice of the [Conceptual Captions](https://ai.google.com/research/ConceptualCaptions/) dataset. These two are downloaded and converted from scratch, but it wouldn't be hard to convert the tutorial to use the caption datasets available in [TensorFlow Datasets](https://www.tensorflow.org/datasets): [Coco Captions](https://www.tensorflow.org/datasets/catalog/coco_captions) and the full [Conceptual Captions](https://www.tensorflow.org/datasets/community_catalog/huggingface/conceptual_captions).\n",
        "\n",
        "Εμείς κατεβάζουμε το flickr30k-images-ecemod το οποίο περιέχει :     \n",
        "  \n",
        "*   ένας φάκελος \"image_dir\" με 31.783 εικόνες από το Flickr\n",
        "*   ένα αρχείο \"captions_new.csv\" με 148.915 captions για τις εικόνες του \"image_dir\"\n",
        "\n",
        "*   ένα αρχείο \"train_files.csv\" λίστα των 21.000 εικόνων που αποτελούν το training set\n",
        "*   ένα αρχείο \"test_files.csv\" λίστα των 4.524 εικόνων που αποτελούν το test set\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBAagBw5p-TM"
      },
      "source": [
        "#### Download the dataset (DEPRICATED)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download with keras: https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file\n",
        "# Download image files\n",
        "# image_zip = tf.keras.utils.get_file('flickr30k-images-ecemod.zip',\n",
        "#                                     cache_subdir=os.path.abspath('.'),\n",
        "#                                     origin='https://spartacus.1337.cx/flickr-mod/flickr30k-images-ecemod.zip',\n",
        "#                                     extract=True)\n",
        "# os.remove(image_zip)"
      ],
      "metadata": {
        "id": "dLBhtwz8VpLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download captions file\n",
        "# (Κάθε caption έχει τρία πεδία, το όνομα του αρχείου της εικόνας, τον αύξοντα αριθμό του caption και τέλος το ίδιο το caption.)\n",
        "# captions_file = tf.keras.utils.get_file('captions_new.csv',\n",
        "#                                         cache_subdir=os.path.abspath('.'),\n",
        "#                                         origin='https://spartacus.1337.cx/flickr-mod/captions_new.csv',\n",
        "#                                         extract=False)\n",
        "\n",
        "# # Download train files list\n",
        "# # (list of image file names)\n",
        "# train_files_list = tf.keras.utils.get_file('train_files.csv',\n",
        "#                                            cache_subdir=os.path.abspath('.'),\n",
        "#                                            origin='https://spartacus.1337.cx/flickr-mod/train_files.csv',\n",
        "#                                            extract=False)\n",
        "\n",
        "# # Download test files list\n",
        "# # (list of image file names)\n",
        "# test_files_list = tf.keras.utils.get_file('test_files.csv',\n",
        "#                                           cache_subdir=os.path.abspath('.'),\n",
        "#                                           origin='https://spartacus.1337.cx/flickr-mod/test_files.csv',\n",
        "#                                           extract=False)"
      ],
      "metadata": {
        "id": "56YwmBxFVsnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import the dataset locally (Alternative)"
      ],
      "metadata": {
        "id": "2hYSm425KrQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CAPTIONS FILE (path)\n",
        "captions_file_path = tf.keras.utils.get_file('captions_new.csv',\n",
        "                                        cache_subdir=os.path.abspath('.'),\n",
        "                                        origin='/captions_new.csv',\n",
        "                                        extract=False)\n",
        "# TRAIN SET FILE (path)\n",
        "train_files_path = tf.keras.utils.get_file('train_files.csv',\n",
        "                                           cache_subdir=os.path.abspath('.'),\n",
        "                                           origin='/train_files.csv',\n",
        "                                           extract=False)\n",
        "# TEST SET FILE (path)\n",
        "test_files_path = tf.keras.utils.get_file('test_files.csv',\n",
        "                                          cache_subdir=os.path.abspath('.'),\n",
        "                                          origin='/test_files.csv',\n",
        "                                          extract=False)\n"
      ],
      "metadata": {
        "id": "hbAiCgX_LIlf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parse the dataset"
      ],
      "metadata": {
        "id": "EISPlyGPW__a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path=\".\"\n",
        "IMAGE_DIR=\"image_dir\"\n",
        "path = pathlib.Path(path)\n",
        "   \n",
        "# READ CAPTIONS\n",
        "captions = (path/captions_file_path).read_text().splitlines() # [image-0#0 \\t caption-0, image-0#1 \\t caption-1, ..., image-N#0 \\t caption-0,]\n",
        "captions = (line.split('\\t') for line in captions)  # [(image-0#0, caption-0), (image-0#1, caption-1), ..., (image-N#0, caption-0)]\n",
        "captions = ((fname.split('#')[0], caption) for (fname, caption) in captions) # [(image-0, caption-0), (image-0, caption-1), ..., (image-N, caption-0)]\n",
        "   \n",
        "# CREATE DICTIONARY OF (image, captions) PAIRS\n",
        "cap_dict = collections.defaultdict(list) # Dictionary of list (key: image file name, value: list of captions for same image)\n",
        "for fname, cap in captions:\n",
        "  cap_dict[fname].append(cap)\n",
        "\n",
        "# READ TRAIN FILES\n",
        "train_files = (path/train_files_path).read_text().splitlines()\n",
        "train_captions = [(str(path/IMAGE_DIR/fname), cap_dict[fname]) for fname in train_files] # list of tuples. Tuple: (image file path, [caption1, caption2, ..])\n",
        "   \n",
        "# READ TEST FILES\n",
        "test_files = (path/test_files_path).read_text().splitlines()\n",
        "test_captions = [(str(path/IMAGE_DIR/fname), cap_dict[fname]) for fname in test_files] # list of tuples. Tuple: (image file path, [caption1, caption2, ..])\n",
        "   \n",
        "# CREATE DATASETS (TRAIN, TEST)\n",
        "train_raw = tf.data.experimental.from_list(train_captions)\n",
        "test_raw = tf.data.experimental.from_list(test_captions)"
      ],
      "metadata": {
        "id": "9PQLp42EWLrn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_raw.element_spec # Type of Dataset elements"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_9BFHelWKBs",
        "outputId": "77c52129-4d29-4849-92ba-775196382d14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
              " TensorSpec(shape=(5,), dtype=tf.string, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIa0ZaP4tBez",
        "outputId": "a57cecd7-7187-4b25-b31a-b86087f452f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'image_dir/_3430497.jpg', shape=(), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'The skier is wearing a yellow jumpsuit and sliding across a yellow rail .'\n",
            " b'A yellow uniformed skier is performing a trick across a railed object .'\n",
            " b'A skier in electric green on the edge of a ramp made of metal bars .'\n",
            " b'A person on skis on a rail at night .'\n",
            " b'A skier slides along a metal rail .'], shape=(5,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "# DATASET SNEAK PEAK\n",
        "for ex_path, ex_captions in train_raw.take(1):\n",
        "  print(ex_path)\n",
        "  print(ex_captions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cSW4u-ORPFQ"
      },
      "source": [
        "### Image feature extractor\n",
        "\n",
        "You will use an image model (pretrained on imagenet) to extract the features from each image. The model was trained as an image classifier, but setting `include_top=False` returns the model without the final classification layer, so you can use the last layer of feature-maps:  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IlUckK8Zfikv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d75d0b2-b040-43f8-bdee-d6631165aea6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_small_224_1.0_float_no_top_v2.h5\n",
            "4334752/4334752 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83683744/83683744 [==============================] - 4s 0us/step\n"
          ]
        }
      ],
      "source": [
        "IMAGE_SHAPE=(224, 224, 3)\n",
        "\n",
        "# Feature Extractor used in Tutorial\n",
        "mobilenet = tf.keras.applications.MobileNetV3Small(\n",
        "    input_shape=IMAGE_SHAPE,\n",
        "    include_top=False, # Exclude final classification layer\n",
        "    include_preprocessing=True\n",
        ")\n",
        "\n",
        "# Feature Extractor \n",
        "xception = tf.keras.applications.Xception(\n",
        "   input_shape=IMAGE_SHAPE,\n",
        "   include_top=False, # Exclude final classification layer\n",
        "   #Error code from -> include_preprocessing=True\n",
        ")\n",
        "\n",
        "mobilenet.trainable=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dojkiou9gL3R"
      },
      "source": [
        "Here's a function to load an image and resize it for the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zXR0217aRPFR"
      },
      "outputs": [],
      "source": [
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SHAPE[:-1])\n",
        "    return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JyQ7zS6gzZh"
      },
      "source": [
        "The model returns a feature map for each image in the input batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sY86n2i6wJNm",
        "outputId": "dd39fe5f-80d2-42bd-f87c-e99315939780",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'image_dir/_3430497.jpg', shape=(), dtype=string)\n",
            "(1, 224, 224, 3)\n",
            "(1, 7, 7, 576)\n"
          ]
        }
      ],
      "source": [
        "print(ex_path)\n",
        "\n",
        "test_img_batch = load_image(ex_path)[tf.newaxis, :] # adds new axis at the front\n",
        "\n",
        "print(test_img_batch.shape)\n",
        "print(mobilenet(test_img_batch).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyqH3zFwRPFi"
      },
      "source": [
        "### Setup the text tokenizer/vectorizer\n",
        "\n",
        "You will transform the text captions into integer sequences using the [TextVectorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) layer, with the following steps:\n",
        "\n",
        "* Use [adapt](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization#adapt) to iterate over all captions, split the captions into words, and compute a vocabulary of the top words.\n",
        "* Tokenize all captions by mapping each word to its index in the vocabulary. All output sequences will be padded to length 50.\n",
        "* Create word-to-index and index-to-word mappings to display results."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### !Text Preproccesing"
      ],
      "metadata": {
        "id": "1imBMzxisrM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "synonyms_list = [\n",
        "    ('several', 'many'),\n",
        "    ('quick', 'fast'),\n",
        "    ('large', 'big'),\n",
        "]\n",
        "numbers_list = ['four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen']"
      ],
      "metadata": {
        "id": "Mf0Tz685sbwI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NroZIzB90hD3"
      },
      "outputs": [],
      "source": [
        "# Reference: https://www.tensorflow.org/text/guide/word_embeddings#:~:text=good%20next%20step.-,Text%20preprocessing,-Next%2C%20define%20the\n",
        "def standardize(s):\n",
        "  s = tf.strings.lower(s)\n",
        "  # Remove punctuation\n",
        "  s = tf.strings.regex_replace(s, f'[{re.escape(string.punctuation)}]', '') # add escape characters\n",
        "  # Avoid Stemming, Removing stop words : For captions to look natural we need stem words in our model training data\n",
        "  # Replace synonyms, verbal numbers, abreviations\n",
        "  for synonym in synonyms_list:\n",
        "    s = tf.strings.regex_replace(s, fr'\\b{synonym[0]}\\b', synonym[1]) # \\b declares the synonym should match the whole word\n",
        "  for number in numbers_list:\n",
        "    s = tf.strings.regex_replace(s, fr'\\b{number}\\b', '')\n",
        "  # Remove whitespaces : the tokenizer removes whitespaces automatically when converting words to integers\n",
        "  s = tf.strings.strip(s)\n",
        "  s = tf.strings.reduce_join(tf.strings.split(s), separator=' ', axis=-1)\n",
        "  # Add start, end words to caption\n",
        "  s = tf.strings.join(['[START]', s, '[END]'], separator=' ') \n",
        "  return s"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "standardize('this is a cat with several legs')"
      ],
      "metadata": {
        "id": "vlSDmKgns8VG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9596d633-b307-4b8e-a635-9168c90867f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=string, numpy=b'[START] this is a cat with many legs [END]'>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "n9SQOXFsyS36"
      },
      "outputs": [],
      "source": [
        "# Use the top X (ex.5000) words for a vocabulary.\n",
        "# Alternatives: 1000, 5000, 2000, 80000, 15000, MAX: 16803\n",
        "vocabulary_size = 5000\n",
        "tokenizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=vocabulary_size,\n",
        "    standardize=standardize,\n",
        "    ragged=True\n",
        ")\n",
        "# Learn the vocabulary from the caption data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "oJGE34aiRPFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30d54312-ee0f-4224-fc59-549d6583e131"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ],
      "source": [
        "tokenizer.adapt(train_raw.map(lambda fp,txt: txt).unbatch().batch(1024))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRahTDtWhJIf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35dd38de-2150-4c2d-d1fc-00537c719fd7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'a', '[START]', '[END]', 'in', 'the', 'on', 'and', 'man']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "tokenizer.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2mGxD33JCxN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10139e18-6e25-41f5-a085-ceced0308ec7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[3, 2, 750, 5, 2, 63, 4], [3, 2, 2857, 34, 4]]>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "t = tokenizer([['a cat in a hat'], ['a robot dog']])\n",
        "t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8Q44tNQVRPFt"
      },
      "outputs": [],
      "source": [
        "# Create mappings for words to indices and indices to words.\n",
        "word_to_index = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=tokenizer.get_vocabulary())\n",
        "index_to_word = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=tokenizer.get_vocabulary(),\n",
        "    invert=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qo-cfCX3LnHs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea994f93-b35b-42cc-9f0c-2fbacfc1d420"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[b'[START]', b'a', b'cat', b'in', b'a', b'hat', b'[END]'],\n",
              " [b'[START]', b'a', b'robot', b'dog', b'[END]']]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "w = index_to_word(t)\n",
        "w.to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrUUfGc65vAT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a66eee82-ee86-42ba-b6e6-45153b738e2f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'[START] a cat in a hat [END]', b'[START] a robot dog [END]'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "tf.strings.reduce_join(w, separator=' ', axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEWM9xrYcg45"
      },
      "source": [
        "### Prepare the datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aX0Z_98S2tN"
      },
      "source": [
        "The `train_raw` and `test_raw` datasets contain 1:many `(image, captions)` pairs. \n",
        "\n",
        "This function will replicate the image so there are 1:1 images to captions:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Replicate images for 1:1 representation"
      ],
      "metadata": {
        "id": "PHYsG7E1XTxu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "3_Lqwl9NiGT0"
      },
      "outputs": [],
      "source": [
        "def match_shapes(images, captions):\n",
        "  caption_shape = einops.parse_shape(captions, 'b c')\n",
        "  captions = einops.rearrange(captions, 'b c -> (b c)')\n",
        "  images = einops.repeat(\n",
        "      images, 'b ... -> (b c) ...',\n",
        "      c = caption_shape['c'])\n",
        "  return images, captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZGUsuGzUfzt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1519e965-9767-496a-dea3-0c6442dda004"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image paths: (32,)\n",
            "captions: (32, 5)\n",
            "\n",
            "image_paths: (160,)\n",
            "captions: (160,)\n"
          ]
        }
      ],
      "source": [
        "# EXAMPLE for replicating 1 batch of photos in the train dataset\n",
        "for ex_paths, ex_captions in train_raw.batch(32).take(1): # batch(32) takes sets of 32 photos each\n",
        "  break\n",
        "\n",
        "print('image paths:', ex_paths.shape)\n",
        "print('captions:', ex_captions.shape)\n",
        "print()\n",
        "\n",
        "ex_paths, ex_captions = match_shapes(images=ex_paths, captions=ex_captions)\n",
        "\n",
        "print('image_paths:', ex_paths.shape)\n",
        "print('captions:', ex_captions.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA1x2j0JXX-N"
      },
      "source": [
        "What do `prepare_images` and `prepare_text` do:\n",
        "\n",
        "1. Load the images (and ignore images that fail to load).\n",
        "2. Replicate images to match the number of captions.\n",
        "3. Shuffle and rebatch the `image, caption` pairs.\n",
        "4. Tokenize the text, shift the tokens and add `label_tokens`.\n",
        "5. Convert the text from a `RaggedTensor` representation to padded dense `Tensor` representation."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extract image features"
      ],
      "metadata": {
        "id": "g15a1aghXgoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_images(ds, image_model, batch_size=32, shuffle_buffer=1000):\n",
        "  # Load the images and make batches.\n",
        "  ds = (ds\n",
        "        .shuffle(10000)                                         # How .shuffle works: https://stackoverflow.com/questions/53514495/what-does-batch-repeat-and-shuffle-do-with-tensorflow-dataset#:~:text=dataset.shuffle(buffer_size%3D3,connected%20to%20the%20source%20dataset.\n",
        "        .map(lambda path, caption: (load_image(path), caption)) # Resize images\n",
        "        .apply(tf.data.experimental.ignore_errors())            # Ignore missing images\n",
        "        .batch(batch_size)                                      # Partition data into batches\n",
        "        .map(lambda images, captions: (image_model(images), captions)) # Get feature maps of images\n",
        "        .map(match_shapes, tf.data.AUTOTUNE)                    # Replicate images for 1:1 representation\n",
        "        .unbatch()\n",
        "        .shuffle(shuffle_buffer)\n",
        "  )\n",
        "  return ds"
      ],
      "metadata": {
        "id": "aZxqvpBDtdbh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenize text + add padding"
      ],
      "metadata": {
        "id": "ghmM6LjdXo-W"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ENR_-swVhnm"
      },
      "source": [
        "To be compatible with keras training the dataset should contain `(inputs, labels)` pairs. For text generation the tokens are both an input and the labels, shifted by one step. This function will convert an `(images, texts)` pair to an `((images, input_tokens), label_tokens)` pair:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "2DsgQ_hZT4C2"
      },
      "outputs": [],
      "source": [
        "def prepare_tokens(imgs, txts):\n",
        "  tokens = tokenizer(txts)\n",
        "\n",
        "  # INITIAL PHRASE\n",
        "  input_tokens = tokens[..., :-1]\n",
        "  print(input_tokens)\n",
        "  # OUTPUT PHRASE\n",
        "  label_tokens = tokens[..., 1:]\n",
        "  print(label_tokens)\n",
        "  return (imgs, input_tokens), label_tokens # TRAIN THE MODEL BASED ON label_tokens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_text(ds, tokenizer, batch_size=32, shuffle_buffer=1000):\n",
        "  def to_tensor(inputs, labels):\n",
        "    (images, in_tok), out_tok = inputs, labels\n",
        "    return (images, in_tok.to_tensor()), out_tok.to_tensor() # Convert RaggedTensor to Tensor\n",
        "\n",
        "  ds = (ds\n",
        "        # .batch(1000)\n",
        "        .map(prepare_tokens, tf.data.AUTOTUNE)   # Prepare input_tokens, label_tokens\n",
        "        # .map(to_tensor, tf.data.AUTOTUNE)     # Add padding to embeddings\n",
        "        # .unbatch()\n",
        "        .shuffle(shuffle_buffer)\n",
        "        .padded_batch(batch_size)             # Add padding to embeddings\n",
        "  )\n",
        "  return ds"
      ],
      "metadata": {
        "id": "eEzK9hEOte8t"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZyKygJ8S8zW"
      },
      "source": [
        "### Cache the image features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_dataset(ds, save_path, shards=10):\n",
        "  # Save the dataset into shard files.\n",
        "  def shard_func(i, item):\n",
        "    return i % shards\n",
        "  ds.enumerate().save(save_path, shard_func=shard_func)"
      ],
      "metadata": {
        "id": "u6YPBseAtXbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(save_path, batch_size=32, shuffle=1000, cycle_length=2):\n",
        "  def custom_reader_func(datasets):\n",
        "    datasets = datasets.shuffle(1000)\n",
        "    return datasets.interleave(lambda x: x, cycle_length=cycle_length)\n",
        "  \n",
        "  ds = tf.data.Dataset.load(save_path, reader_func=custom_reader_func)\n",
        "\n",
        "  def drop_index(i, x):\n",
        "    return x\n",
        "\n",
        "  ds = (ds\n",
        "        .map(drop_index, tf.data.AUTOTUNE)\n",
        "        # .shuffle(shuffle)\n",
        "        .prefetch(tf.data.AUTOTUNE)   # Prepare later elements, while the current is being processed. Improves latency, throughput.\n",
        "  )  \n",
        "  \n",
        "  return ds"
      ],
      "metadata": {
        "id": "apT-BZeNtaS5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHKhSKhti6NS"
      },
      "source": [
        "Since the image feature extractor is not changing, and this tutorial is not using image augmentation, the image features can be cached. Same for the text tokenization. The time it takes to set up the cache is earned back on each epoch during training and validation. The code below defines two functions `save_dataset` and `load_dataset`: "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ds_images_train = prepare_images(train_raw, mobilenet)\n",
        "# ds_images_test = prepare_images(test_raw, mobilenet)"
      ],
      "metadata": {
        "id": "lx8899_Htrv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save_dataset(ds_images_train, 'train_cache_features') # Saves (image features, captions) ex. ds -> (image_1_features, caption_1), (image_1_features, caption_2), ...\n",
        "# save_dataset(ds_images_test, 'test_cache_features')"
      ],
      "metadata": {
        "id": "0nseOSdctvL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "798DtfH51UI8"
      },
      "source": [
        " </section>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI265LiDslr2"
      },
      "source": [
        "## Data ready for training\n",
        "\n",
        "After those preprocessing steps, here are the datasets:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds_features = load_dataset('train_cache_features')\n",
        "test_ds_features = load_dataset('test_cache_features')"
      ],
      "metadata": {
        "id": "v9cEmxMEapIL"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the tokenized captions\n",
        "train_ds = prepare_text(train_ds_features, tokenizer) # ((image_features, input_tokens), output_tokens)\n",
        "test_ds = prepare_text(test_ds_features, tokenizer)"
      ],
      "metadata": {
        "id": "EUnACPdeue76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "529025a1-d65b-4984-b694-d33a91984b36"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"strided_slice:0\", shape=(None,), dtype=int64)\n",
            "Tensor(\"strided_slice_1:0\", shape=(None,), dtype=int64)\n",
            "Tensor(\"strided_slice:0\", shape=(None,), dtype=int64)\n",
            "Tensor(\"strided_slice_1:0\", shape=(None,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3B80JXj7HloX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01876cea-78dd-4cd3-a3da-3f6657d12894"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((TensorSpec(shape=(None, 7, 7, 576), dtype=tf.float32, name=None),\n",
              "  TensorSpec(shape=(None, None), dtype=tf.int64, name=None)),\n",
              " TensorSpec(shape=(None, None), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "train_ds.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jfb8qknlsKi"
      },
      "source": [
        "The dataset now returns `(input, label)` pairs suitable for training with keras. The `inputs` are `(images, input_tokens)` pairs. The `images` have been processed with the feature-extractor model. For each location in the `input_tokens` the model looks at the text so far and tries to predict the next which is lined up at the same location in the `labels`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJBEwuXLZQdw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29f5f371-0ca3-4d2a-c8c5-bb41b2d4bda2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 7, 7, 576)\n",
            "(32, 26)\n",
            "(32, 26)\n"
          ]
        }
      ],
      "source": [
        "for (inputs, ex_labels) in train_ds.take(1):\n",
        "  (ex_img, ex_in_tok) = inputs\n",
        "\n",
        "print(ex_img.shape)\n",
        "print(ex_in_tok.shape)\n",
        "print(ex_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22R58DzZoF17"
      },
      "source": [
        "The input tokens and the labels are the same, just shifted by 1 step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7h5UGftn1hT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6286b260-190b-464a-b467-71c62694af4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   3    2  610 1948    1  110   10  294  149   38    7    6  492    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0]\n",
            "[   2  610 1948    1  110   10  294  149   38    7    6  492    4    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0]\n"
          ]
        }
      ],
      "source": [
        "print(ex_in_tok[0].numpy())\n",
        "print(ex_labels[0].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfICM49WFpIb"
      },
      "source": [
        "## A Transformer decoder model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONyjuWsmZoyO"
      },
      "source": [
        "This model assumes that the pretrained image encoder is sufficient, and just focuses on building the text decoder. This tutorial uses a 2-layer Transformer-decoder.\n",
        "\n",
        "The implementations are almost identical to those in the [Transformers tutorial](https://www.tensorflow.org/text/tutorials/transformer). Refer back to it for more details.\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th>The Transformer encoder and decoder.</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=250 src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-1layer-words.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiRXWwIKNybB"
      },
      "source": [
        "The model will be implemented in three main parts: \n",
        "\n",
        "1. Input - The token embedding and positional encoding (`SeqEmbedding`).\n",
        "1. Decoder - A stack of transformer decoder layers (`DecoderLayer`) where each contains:\n",
        "   1. A causal self attention later (`CausalSelfAttention`), where each output location can attend to the output so far.\n",
        "   1. A cross attention layer (`CrossAttention`) where each output location can attend to the input image.\n",
        "   1. A feed forward network (`FeedForward`) layer which further processes each output location independently.\n",
        "1. Output - A multiclass-classification over the output vocabulary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ngm3SQMCaYU"
      },
      "source": [
        "### Input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9suaARZGPKw"
      },
      "source": [
        "The input text has already been split up into tokens and converted to sequences of IDs. \n",
        "\n",
        "Remember that unlike a CNN or RNN the Transformer's attention layers are invariant to the order of the sequence. Without some positional input, it just sees an unordered set not a sequence. So in addition to a simple vector embedding for each token ID, the embedding layer will also include an embedding for each position in the sequence.\n",
        "\n",
        "The `SeqEmbedding` layer defined below:\n",
        "\n",
        "- It looks up the embedding vector for each token.\n",
        "- It looks up an embedding vector for each sequence location.\n",
        "- It adds the two together."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### !Pretrained GloVe-wiki embeddings"
      ],
      "metadata": {
        "id": "TJGxl4EKofZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Τώρα θα χρησιμοποιήσουμε pretrained glove-wiki embeddings, καθώς μπορούμε να επωφεληθούμε από το γεγονός ότι έχουν προκύψει ύστερα από εκπαίδευση σε πολύ μεγάλα σύνολα δεδομένων και από το γεγονός ότι μπορούμε να εξοικονομήσουμε χρόνο εκπαίδευσης, αφού αυτό το επίπεδο δεν θα απαιτεί πλέον εκπαίδευση.\n",
        "\n",
        "Με την χρήση των προεκπαιδευμένων μοντέλων embeddings μπορούμε να παρατηρήσουμε μια μικρή βελτίωση της απόδοσης του μοντέλου και μια σημαντική μείωση του χρόνου εκπαίδευσης."
      ],
      "metadata": {
        "id": "dtmngDBEo62V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader \n",
        "\n",
        "#Όλα τα διαθέσιμα pretrained μοντέλα που μπορούμε να χρησιμοοποιήσουμε\n",
        "for i in list(gensim.downloader.info()['models'].keys()):\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "4JF_AItXhUPf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a29502f6-93eb-4720-ecf3-b932d51cfa22"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fasttext-wiki-news-subwords-300\n",
            "conceptnet-numberbatch-17-06-300\n",
            "word2vec-ruscorpora-300\n",
            "word2vec-google-news-300\n",
            "glove-wiki-gigaword-50\n",
            "glove-wiki-gigaword-100\n",
            "glove-wiki-gigaword-200\n",
            "glove-wiki-gigaword-300\n",
            "glove-twitter-25\n",
            "glove-twitter-50\n",
            "glove-twitter-100\n",
            "glove-twitter-200\n",
            "__testing_word2vec-matrix-synopsis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Διαλέξτε ένα απο τα παρακάτω embeddings\n",
        "\n",
        "#glove_wiki_50 = gensim.downloader.load('glove-wiki-gigaword-50')\n",
        "#glove_wiki_100 = gensim.downloader.load('glove-wiki-gigaword-100')\n",
        "# glove_wiki_200 = gensim.downloader.load('glove-wiki-gigaword-200')\n",
        "glove_wiki_300 = gensim.downloader.load('glove-wiki-gigaword-300')"
      ],
      "metadata": {
        "id": "X8fnlmC-hYPz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "468940b7-c296-4730-983d-5fc0d56a9f5c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ανάλογα με την επιλογή μας αλλάζουμε τις παρακάτω παραμέτρους \n",
        "embed_model = glove_wiki_300\n",
        "EMBEDDING_DIM = 300"
      ],
      "metadata": {
        "id": "QfuYrl2Vh2bL"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ο πίνακας embedding_matrix μας παρέχει μια αντιστοιχία 1-1 μεταξύ μιας λέξης και της ενσωμάτωσής της. Για τις λέξεις που δεν περιλαμβάνονται στο μοντέλο (glove-wiki-x), οι οποίες πρέπει να είναι λίγες, τις αντιστοιχίζουμε σε μηδενικό διάνυσμα."
      ],
      "metadata": {
        "id": "-pkaD9Smi79O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCABULARY_SIZE = tokenizer.vocabulary_size() # = 5000\n",
        "# Αρχικοποιούμε το πίνακά μας στις διαστάσεις του μοντέλου\n",
        "embedding_matrix = np.zeros((VOCABULARY_SIZE, EMBEDDING_DIM))\n",
        " \n",
        "for idx, word in enumerate(tokenizer.get_vocabulary()):\n",
        "    try:\n",
        "        embedding_vector = embed_model[word]\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "    except KeyError:\n",
        "        vec = np.zeros(EMBEDDING_DIM)\n",
        "        embedding_matrix[idx] = vec"
      ],
      "metadata": {
        "id": "bYxwNqsAiBzu"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix.shape"
      ],
      "metadata": {
        "id": "bWP6A8dSqjAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b354a23f-b2de-4422-efcc-3124b0fdaded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "P91LU2F0a9Ga"
      },
      "outputs": [],
      "source": [
        "class SeqEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, max_length, depth):\n",
        "    super().__init__()\n",
        "     # Depth = Embedding Dimension Output = Units\n",
        "    self.pos_embedding = tf.keras.layers.Embedding(input_dim=max_length, output_dim=depth)\n",
        "\n",
        "    self.token_embedding = tf.keras.layers.Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=depth,\n",
        "        weights=[embedding_matrix], #Χρησιμοποιούμε τα έτοιμα βάρη\n",
        "        trainable=False)\n",
        "    \n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "  def call(self, seq):\n",
        "    seq = self.token_embedding(seq) # (batch, seq, depth)\n",
        "\n",
        "    x = tf.range(tf.shape(seq)[1])  # (seq)\n",
        "    x = x[tf.newaxis, :]  # (1, seq)\n",
        "    x = self.pos_embedding(x)  # (1, seq, depth)\n",
        "\n",
        "    return self.add([seq,x])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II1mD-bBCdMB"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHMLeMtKPTCW"
      },
      "source": [
        "The decoder is a standard Transformer-decoder, it contains a stack of `DecoderLayers` where each contains three sublayers: a `CausalSelfAttention`, a `CrossAttention`, and a`FeedForward`. The implementations are almost identical to the [Transformer tutorial](https://www.tensorflow.org/text/tutorials/transformer), refer to it for more details.\n",
        "\n",
        "The `CausalSelfAttention` layer is below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "6JTLiX3lKooQ"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    # Use Add instead of + so the keras mask propagates through.\n",
        "    self.add = tf.keras.layers.Add() \n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "  \n",
        "  def call(self, x):\n",
        "    attn = self.mha(query=x, value=x,\n",
        "                    use_causal_mask=True)\n",
        "    x = self.add([x, attn])\n",
        "    return self.layernorm(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c66OTRwQfd8"
      },
      "source": [
        "The `CrossAttention` layer is below. Note the use of `return_attention_scores`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "rIY6Vu2pLBAO"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self,**kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.add = tf.keras.layers.Add() \n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "  \n",
        "  def call(self, x, y, **kwargs):\n",
        "    attn, attention_scores = self.mha(\n",
        "             query=x, value=y,\n",
        "             return_attention_scores=True)\n",
        "    \n",
        "    self.last_attention_scores = attention_scores\n",
        "\n",
        "    x = self.add([x, attn])\n",
        "    return self.layernorm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Hn5p6f-RE0C"
      },
      "source": [
        "The `FeedForward` layer is below. Remember that a `layers.Dense` layer is applied to the last axis of the input. The input will have a shape of `(batch, sequence, channels)`, so it automatically applies pointwise across the `batch` and `sequence` axes.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "cWKrl7teOnH2"
      },
      "outputs": [],
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(units=2*units, activation='relu'),\n",
        "        tf.keras.layers.Dense(units=units),\n",
        "        tf.keras.layers.Dropout(rate=dropout_rate),\n",
        "    ])\n",
        "\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "  \n",
        "  def call(self, x):\n",
        "    x = x + self.seq(x)\n",
        "    return self.layernorm(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbXoiVNPRoJc"
      },
      "source": [
        "Next arrange these three layers into a larger `DecoderLayer`. Each decoder layer applies the three smaller layers in sequence. After each sublayer the shape of `out_seq` is `(batch, sequence, channels)`. The decoder layer also returns the `attention_scores` for later visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ydcW5KZZHou7"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, num_heads=1, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.self_attention = CausalSelfAttention(num_heads=num_heads,\n",
        "                                              key_dim=units,\n",
        "                                              dropout=dropout_rate)\n",
        "    self.cross_attention = CrossAttention(num_heads=num_heads,\n",
        "                                          key_dim=units,\n",
        "                                          dropout=dropout_rate)\n",
        "    self.ff = FeedForward(units=units, dropout_rate=dropout_rate)\n",
        "      \n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    in_seq, out_seq = inputs\n",
        "\n",
        "    # Text input\n",
        "    out_seq = self.self_attention(out_seq)\n",
        "\n",
        "    out_seq = self.cross_attention(out_seq, in_seq)\n",
        "    \n",
        "    self.last_attention_scores = self.cross_attention.last_attention_scores\n",
        "\n",
        "    out_seq = self.ff(out_seq)\n",
        "\n",
        "    return out_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lgbYrF5Csqu"
      },
      "source": [
        "### Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcnKZkrklAQf"
      },
      "source": [
        "At minimum the output layer needs a `layers.Dense` layer to generate logit-predictions for each token at each location."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WQD87efena5"
      },
      "source": [
        "But there are a few other features you can add to make this work a little better:\n",
        "\n",
        "1. **Handle bad tokens**: The model will be generating text. It should\n",
        "   never generate a pad, unknown, or start token (`''`, `'[UNK]'`, \n",
        "   `'[START]'`). So set the bias for these to a large negative value.\n",
        "\n",
        "   > Note: You'll need to ignore these tokens in the loss function as well. \n",
        "\n",
        "2. **Smart initialization**: The default initialization of a dense layer will\n",
        "  give a model that initially predicts each token with almost uniform\n",
        "  likelihood. The actual token distribution is far from uniform. The\n",
        "  optimal value for the initial bias of the output layer is the log of the\n",
        "  probability of each token. So include an `adapt` method to count the tokens\n",
        "  and set the optimal initial bias. This reduces the initial loss from the\n",
        "  entropy of the uniform distribution (`log(vocabulary_size)`) to the marginal\n",
        "  entropy of the distribution (`-p*log(p)`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "CeWw2SFDHUfo"
      },
      "outputs": [],
      "source": [
        "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization\n",
        "class TokenOutput(tf.keras.layers.Layer):\n",
        "  def __init__(self, tokenizer, banned_tokens=('', '[UNK]', '[START]'), **kwargs):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(\n",
        "        units=tokenizer.vocabulary_size(), **kwargs)\n",
        "    self.tokenizer = tokenizer\n",
        "    self.banned_tokens = banned_tokens\n",
        "\n",
        "    self.bias = None\n",
        "\n",
        "  def adapt(self, ds):\n",
        "    counts = collections.Counter()\n",
        "    vocab_dict = {name: id \n",
        "                  for id, name in enumerate(self.tokenizer.get_vocabulary())}\n",
        "\n",
        "    for tokens in tqdm.tqdm(ds):\n",
        "      counts.update(tokens.numpy().flatten())\n",
        "\n",
        "    counts_arr = np.zeros(shape=(self.tokenizer.vocabulary_size(),))\n",
        "    counts_arr[np.array(list(counts.keys()), dtype=np.int32)] = list(counts.values())\n",
        "\n",
        "    counts_arr = counts_arr[:]\n",
        "    for token in self.banned_tokens:\n",
        "      counts_arr[vocab_dict[token]] = 0\n",
        "\n",
        "    total = counts_arr.sum()\n",
        "    p = counts_arr/total\n",
        "    p[counts_arr==0] = 1.0\n",
        "    log_p = np.log(p)  # log(1) == 0\n",
        "\n",
        "    entropy = -(log_p*p).sum()\n",
        "\n",
        "    print()\n",
        "    print(f\"Uniform entropy: {np.log(self.tokenizer.vocabulary_size()):0.2f}\")\n",
        "    print(f\"Marginal entropy: {entropy:0.2f}\")\n",
        "\n",
        "    self.bias = log_p\n",
        "    self.bias[counts_arr==0] = -1e9\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.dense(x)\n",
        "    # TODO(b/250038731): Fix this.\n",
        "    # An Add layer doesn't work because of the different shapes.\n",
        "    # This clears the mask, that's okay because it prevents keras from rescaling\n",
        "    # the losses.\n",
        "    return x + self.bias\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzQHqANd1A6Q"
      },
      "source": [
        "The smart initialization will significantly reduce the initial loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "GGnOQyc501B2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afe93dcc-73f7-48c9-9cf0-3e36bafa65e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1784/1784 [01:08<00:00, 26.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Uniform entropy: 8.52\n",
            "Marginal entropy: 5.46\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "output_layer = TokenOutput(tokenizer, banned_tokens=('', '[UNK]', '[START]'))\n",
        "# This might run a little faster if the dataset didn't also have to load the image data.\n",
        "output_layer.adapt(train_ds.map(lambda inputs, labels: labels)) # Create a vocabulary from the existing captions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gq-ICN7bD-u"
      },
      "source": [
        "### Build the model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Captioner Class"
      ],
      "metadata": {
        "id": "7UtH10UJorR6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gou4fPH_SWgH"
      },
      "source": [
        "To build the model, you need to combine several parts:\n",
        "\n",
        "1. The image `feature_extractor` and the text `tokenizer` and.\n",
        "1. The `seq_embedding` layer, to convert batches of token-IDs to \n",
        "   vectors `(batch, sequence, channels)`.\n",
        "3. The stack of `DecoderLayers` layers that will process the text and image data.\n",
        "4. The `output_layer` which returns a pointwise prediction of what the next word should be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "bHCISYehH1f6"
      },
      "outputs": [],
      "source": [
        "class Captioner(tf.keras.Model):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self, tokenizer, feature_extractor, output_layer, num_layers=1,\n",
        "               units=256, max_length=50, num_heads=1, dropout_rate=0.1): # max_length = 100\n",
        "    super().__init__()\n",
        "    self.feature_extractor = feature_extractor\n",
        "    self.tokenizer = tokenizer\n",
        "    self.word_to_index = tf.keras.layers.StringLookup(\n",
        "        mask_token=\"\",\n",
        "        vocabulary=tokenizer.get_vocabulary())\n",
        "    self.index_to_word = tf.keras.layers.StringLookup(\n",
        "        mask_token=\"\",\n",
        "        vocabulary=tokenizer.get_vocabulary(),\n",
        "        invert=True) \n",
        "\n",
        "    self.seq_embedding = SeqEmbedding(\n",
        "        vocab_size=tokenizer.vocabulary_size(),\n",
        "        depth=units, # = Embedding_Dimension_Output\n",
        "        max_length=max_length)\n",
        "\n",
        "    self.decoder_layers = [\n",
        "        DecoderLayer(units, num_heads=num_heads, dropout_rate=dropout_rate)\n",
        "        for n in range(num_layers)]\n",
        "\n",
        "    self.output_layer = output_layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW390dOz9T-x"
      },
      "source": [
        "When you call the model, for training, it receives an `image, txt` pair. To make this function more usable, be flexible about the input:\n",
        "\n",
        "* If the image has 3 channels run it through the feature_extractor. Otherwise assume that it has been already. Similarly\n",
        "* If the text has dtype `tf.string` run it through the tokenizer.\n",
        "\n",
        "After that running the model is only a few steps:\n",
        "\n",
        "1. Flatten the extracted image features, so they can be input to the decoder layers.\n",
        "2. Look up the token embeddings.\n",
        "3. Run the stack of `DecoderLayer`s, on the image features and text embeddings.\n",
        "4. Run the output layer to predict the next token at each position.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "lPdb7I4h9Ulo"
      },
      "outputs": [],
      "source": [
        "@Captioner.add_method\n",
        "def call(self, inputs):\n",
        "  image, txt = inputs\n",
        "\n",
        "  if image.shape[-1] == 3:\n",
        "    # Apply the feature-extractor, if you get an RGB image.\n",
        "    image = self.feature_extractor(image)\n",
        "  \n",
        "  # Flatten the feature map\n",
        "  image = einops.rearrange(image, 'b h w c -> b (h w) c')\n",
        "\n",
        "\n",
        "  if txt.dtype == tf.string:\n",
        "    # Apply the tokenizer if you get string inputs.\n",
        "    txt = tokenizer(txt)\n",
        "\n",
        "  txt = self.seq_embedding(txt)\n",
        "\n",
        "  # Look at the image\n",
        "  for dec_layer in self.decoder_layers:\n",
        "    txt = dec_layer(inputs=(image, txt))\n",
        "    \n",
        "  txt = self.output_layer(txt)\n",
        "\n",
        "  return txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### !Υπερπαράμετροι του Decoder\n",
        "Παρατηρούμε αύξηση του corpus_bleu score όσο αυξάνουμε των αριθμό των layers και heads, όπως είναι αναμενόμενο, όμως με αυτόν τον τρόπο αυξάνεται η πολυπλοκότητα του μοντέλου, συνεπώς και ο χρονος εκπαίδευσης του. \n",
        "Στη μεταβλητή units θέτουμε την τιμή 300, δηλαδή την τιμή διάστασης του μοντέλου που επιλέξαμε για τα Embeddings. Και πάλι όπως είναι αναμενόμενο, με μεγαλύτερη διάσταση μοντέλου Embeddings έχουμε καλύτερα αποτελέσματα. Τέλος το dropout rate βγάζει βέλτιστα αποτελέσματα σε τιμές κοντά στο 0.5."
      ],
      "metadata": {
        "id": "8aOkyC0q0t4w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "kmM7aZQsLiyU"
      },
      "outputs": [],
      "source": [
        "# Θέτουμε την μεταβλητή units ανάλογα με τη διάσταση του μοντέλου που διαλέξαμε (Embedding_Dimension)\n",
        "model = Captioner(\n",
        "    tokenizer, \n",
        "    feature_extractor=mobilenet, \n",
        "    output_layer=output_layer, \n",
        "    units=EMBEDDING_DIM, \n",
        "    dropout_rate=0.5, \n",
        "    num_layers=8, \n",
        "    num_heads=10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGvOcLQKghXN"
      },
      "source": [
        "### Generate captions\n",
        "\n",
        "Before getting into training, write a bit of code to generate captions. You'll use this to see how training is progressing.\n",
        "\n",
        "Start by downloading a test image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "cwFcdMqC-jE2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11a584ca-76d2-40e3-c9b8-d97e420b77dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://tensorflow.org/images/surf.jpg\n",
            "64400/64400 [==============================] - 0s 2us/step\n"
          ]
        }
      ],
      "source": [
        "image_url = 'https://tensorflow.org/images/surf.jpg'\n",
        "image_path = tf.keras.utils.get_file('surf.jpg', origin=image_url)\n",
        "image = load_image(image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### !Beam Search generation"
      ],
      "metadata": {
        "id": "o68RlseOvjUC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Beam Search parameter b determines how many branches / sentences we choose in each stem of the sentence generation process. <br>\n",
        "Usually, the bigger the parameter b the better the accuracy of the model, but the longer it takes for the captions generation.\n",
        "Usually, a parameter b=5 is a good tradeoff between time and performance. <br>\n",
        "Here we use a default value of b=2 in order to speed up the process of finding the best hyperparameters. <br> \n",
        "(The value b=3 gives us equally good results with value b=5, so we choose that)"
      ],
      "metadata": {
        "id": "KbJncqXVCpBJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JefwCRZ8z-Ah"
      },
      "source": [
        "The **temperature** parameter allows you to interpolate between 3 modes:\n",
        "\n",
        "1. Greedy decoding (`temperature=0.0`) - Chooses the most likely next token at each step.\n",
        "2. Random sampling according to the logits (`temperature=1.0`).\n",
        "3. Uniform random sampling (`temperature >> 1.0`). \n",
        "\n",
        "Since the model is untrained, and it used the frequency-based initialization, the \"greedy\" output (first) usually only contains the most common tokens: `['a', '.', '[END]']`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Nf1Jie9ef_Cg"
      },
      "outputs": [],
      "source": [
        "@Captioner.add_method\n",
        "def simple_gen(self, image, beam_search=True, beam_size=2, temperature=1):\n",
        "\n",
        "  initial = self.word_to_index([['[START]']])                        # 'START' token == 3\n",
        "  if image.shape[-1] == 3:\n",
        "    img_features = self.feature_extractor(image[tf.newaxis, ...])    # extract features from image\n",
        "  else:\n",
        "    img_features = image[tf.newaxis, :]\n",
        "\n",
        "  def beam_search_gen():\n",
        "    tokens_list = [(initial, 0.0)]                  # list of tuples (tokens, propability): stores the top b sentences for each branch (in each iteration)\n",
        "    final_tokens_list = []                          # list of tuples (tokens, propability): final top b selected sentences\n",
        "\n",
        "    remaining_beam_size = beam_size                 # Counter for how many branches in the beam search tree have not terminated yet\n",
        "    for n in range(50):\n",
        "      if remaining_beam_size == 0:\n",
        "        break\n",
        "      new_tokens_list = []                          # top_b tokens for each branch (b*b tokens)\n",
        "      for tokens, prob in tokens_list:              # tokens: [3, 15, 22, 10, ...], prob: float (ex. 0.23)\n",
        "        if tokens[0][-1] == self.word_to_index('[END]') or n == 49:\n",
        "          remaining_beam_size -= 1\n",
        "          final_tokens_list.append((tokens, prob))\n",
        "          continue\n",
        "        preds = self((img_features, tokens)).numpy()    # generate probabilities for all words given a sentence 'tokens' and an image 'img_features'\n",
        "        preds = preds[:, -1, :]                         # get the probabilities vector only for the last word\n",
        "        top_b_preds = tf.math.top_k(preds, k=remaining_beam_size) # get the b words with the biggest probability\n",
        "        top_b_tokens = top_b_preds.indices              # tokens \n",
        "        top_b_prob = top_b_preds.values                 # probabilities\n",
        "        for i in range(remaining_beam_size):\n",
        "          next_token = [[top_b_tokens[0][i]]]           # next token to be added to the sentence (in one branch)\n",
        "          new_tokens = tf.concat([tokens, next_token], axis=1)\n",
        "          new_log_prob = prob + top_b_prob[0][i]\n",
        "          new_tokens_list.append((new_tokens, new_log_prob))\n",
        "        \n",
        "      tokens_list = sorted(new_tokens_list, key=lambda x: -x[1])[:remaining_beam_size] # Sort the list with b*b sentences and keep the first b\n",
        "\n",
        "    # Return the sentence with the biggest (probability / sentence length) score\n",
        "    # print(final_tokens_list)\n",
        "    tokens, prob = max(final_tokens_list, key=lambda x: x[1]/len(x[0][0, 1:-1]))\n",
        "    words = index_to_word(tokens[0, 1:-1])\n",
        "    result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "    return result.numpy().decode()\n",
        "  \n",
        "  # ============================================================================\n",
        "  def temperature_gen():\n",
        "    tokens = initial # (batch, sequence)\n",
        "    for n in range(50):\n",
        "      # 1. Get predictions given the previous words and the features\n",
        "      preds = self((img_features, tokens)).numpy()  # (batch, sequence, vocab) -> shape (1, sentence length , list of values for each word)\n",
        "      # print(preds.shape) # -> ex. (1, 4, 5000)\n",
        "      # 2. Get the last generated vector\n",
        "      preds = preds[:,-1, :]  #(batch, vocab) -> take the last vector from the sentence\n",
        "      # print(preds.shape) # -> ex. (1, 5000)\n",
        "      # 3. Choose the next word based on the temperature parameter\n",
        "      if temperature==0:\n",
        "          next = tf.argmax(preds, axis=-1)[:, tf.newaxis]  # (batch, 1)\n",
        "      else:\n",
        "          next = tf.random.categorical(preds/temperature, num_samples=1)  # (batch, 1)\n",
        "      # 4. Add the selected word to the sentence\n",
        "      tokens = tf.concat([tokens, next], axis=1) # (batch, sequence) \n",
        "\n",
        "      if next[0] == self.word_to_index('[END]'):\n",
        "        break\n",
        "    words = index_to_word(tokens[0, 1:-1])\n",
        "    result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "    return result.numpy().decode()\n",
        "\n",
        "  if (beam_search):\n",
        "    return beam_search_gen()\n",
        "  else:\n",
        "    return temperature_gen()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxN2NPX2zB8y"
      },
      "source": [
        "Here are some generated captions for that image, the model's untrained, so they don't make much sense yet:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.simple_gen(image, beam_size=3)"
      ],
      "metadata": {
        "id": "TpBK9e7CvztH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "0d8d6cc1-f83e-46d2-89b8-42a8f2530d36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPm96CccvHnq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6cc11e5-7258-4bc0-901e-bdce38d60b95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n",
            "a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n",
            "<class 'str'>\n",
            "a a a a a a a through playing a a a a a a a a man a a a a woman a a a a\n",
            "<class 'str'>\n",
            "a team motorcycle in pants of many hanging dusk man bright girls in grass\n"
          ]
        }
      ],
      "source": [
        "for t in (0.0, 0.5, 1.0):\n",
        "  result = model.simple_gen(image, False, temperature=t)\n",
        "  print(type(result))\n",
        "  print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0FpTvaPkqON"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKcwZdqObK-U"
      },
      "source": [
        "To train the model you'll need several additional components:\n",
        "\n",
        "- The Loss and metrics\n",
        "- The Optimizer\n",
        "- Optional Callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare datasets for metrics / validation"
      ],
      "metadata": {
        "id": "72sBNvaW42ZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAIN SET: (image_path, [caption 1, caption 2, ...]) -> (image, [caption 1, caption 2, ...])\n",
        "train_raw_filtered = (train_raw\n",
        "        .map(lambda path, caption: (load_image(path), caption)) # Resize images\n",
        "        .apply(tf.data.experimental.ignore_errors())            # Ignore missing images\n",
        ")\n",
        "# TEST SET: (image_path, [caption 1, caption 2, ...]) -> (image, [caption 1, caption 2, ...])\n",
        "test_raw_filtered = (test_raw\n",
        "        .map(lambda path, caption: (load_image(path), caption)) # Resize images\n",
        "        .apply(tf.data.experimental.ignore_errors())            # Ignore missing images\n",
        ")"
      ],
      "metadata": {
        "id": "MPjXo9iq5AOB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cc678e2-04b7-4f38-d2aa-7377a3776617"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-43-8e5ac48cd6f7>:4: ignore_errors (from tensorflow.python.data.experimental.ops.error_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.ignore_errors` instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5IW2mWa2sAG"
      },
      "source": [
        "### Losses and metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk.translate.bleu_score as bleu_score\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction"
      ],
      "metadata": {
        "id": "7j9gi6_SDhmo"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbpbDQTw1lOW"
      },
      "source": [
        "Here's an implementation of a masked loss and accuracy:\n",
        "\n",
        "When calculating the mask for the loss, note the `loss < 1e8`. This term discards the artificial, impossibly high losses for the `banned_tokens`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "s24im3FqxAfT"
      },
      "outputs": [],
      "source": [
        "def masked_loss(labels, preds):  \n",
        "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, preds)\n",
        "\n",
        "  mask = (labels != 0) & (loss < 1e8) \n",
        "  mask = tf.cast(mask, loss.dtype)\n",
        "\n",
        "  loss = loss*mask\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "def masked_acc(labels, preds):\n",
        "  mask = tf.cast(labels!=0, tf.float32)\n",
        "  preds = tf.argmax(preds, axis=-1)\n",
        "  labels = tf.cast(labels, tf.int64)\n",
        "  match = tf.cast(preds == labels, mask.dtype)\n",
        "  acc = tf.reduce_sum(match*mask)/tf.reduce_sum(mask)\n",
        "  return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### !Bleu Score"
      ],
      "metadata": {
        "id": "gFesxYCjY86n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Υλοποιούμε 3 συναρτήσεις για την αξιολόγηση του μοντέλου μας μέσω της BLEU metric.\n",
        "\n",
        "Η **sentence_bleu_score** λαμβάνει ως ορίσματα τις ετικέτες του dataset και τις προβλεπόμενες ετικέτες απο το μοντέλο και επιστρέφει το bleu score για την εικόνα με τα labels αυτά.\n",
        "\n",
        "Η **evaluate_bleu** λαμβάνει ως όρισμα ένα index της εικόνας στο Test Dataset, την οποία θέλουμε να αξιολογήσουμε, και υπολογίζει το bleu score της χρησιμοποιώντας την συνάρτηση sentence_bleu.\n",
        "\n",
        "Η **batch_bleu** λαμβάνει ως όρισμα το μέγεθος του batch από εικόνες του Dataset το οποίο θέλουμε να αξιολογήσουμε και χρησιμοποιώντας την corpus_bleu επιστρέφει το bleu score του batch."
      ],
      "metadata": {
        "id": "EyhBVsJj3C25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_bleu_score(labels, preds):\n",
        "  weights = (0.4, 0.3, 0.2, 0.1)\n",
        "  smoothing_function=SmoothingFunction().method1\n",
        "  # labels: Tensor(['caption 1', 'caption 2', 'caption 3', ...])\n",
        "  labels = labels.numpy().tolist()                                    # Cast tensor to list\n",
        "  labels = [item[:-1].decode().lower().split() for item in labels]    # Split sentence into list of words + remove . from end\n",
        "  preds = preds.split()                                               # Turn predictions into list of words\n",
        "  print(labels)\n",
        "  print(preds)\n",
        "\n",
        "  bleu_score = sentence_bleu(\n",
        "      labels,\n",
        "      preds,\n",
        "      weights = weights, \n",
        "      smoothing_function = smoothing_function\n",
        "  )\n",
        "  return bleu_score\n",
        "  "
      ],
      "metadata": {
        "id": "i2qaMU4fyYsc"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ex_image, ex_captions in train_raw_filtered.take(1):\n",
        "  ex_output = model.simple_gen(ex_image)\n",
        "  bleu = sentence_bleu_score(ex_captions, ex_output)\n",
        "  print(bleu)"
      ],
      "metadata": {
        "id": "ykh-0vJDDHyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57dfa852-07f3-461a-c908-a9f355d4ca8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['the', 'skier', 'is', 'wearing', 'a', 'yellow', 'jumpsuit', 'and', 'sliding', 'across', 'a', 'yellow', 'rail'], ['a', 'yellow', 'uniformed', 'skier', 'is', 'performing', 'a', 'trick', 'across', 'a', 'railed', 'object'], ['a', 'skier', 'in', 'electric', 'green', 'on', 'the', 'edge', 'of', 'a', 'ramp', 'made', 'of', 'metal', 'bars'], ['a', 'person', 'on', 'skis', 'on', 'a', 'rail', 'at', 'night'], ['a', 'skier', 'slides', 'along', 'a', 'metal', 'rail']]\n",
            "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a']\n",
            "0.008295638755017072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_bleu(input):\n",
        "  iterator = iter(test_raw_filtered)\n",
        "  for nr in range(input):\n",
        "    test_raw_instance = iterator.get_next()\n",
        "\n",
        "  img, labels = test_raw_instance\n",
        "  result = model.simple_gen(img, beam_size=2, temperature=0.0) # to use temperature metric : simple_gen(img, False, temperature=0)\n",
        "\n",
        "  labels = str(tf.strings.reduce_join(labels.numpy().tolist(), separator=' ', axis=-1).numpy())\n",
        "  labels = labels.split('.')\n",
        "  labels = [item.lower().split() for item in labels[:-1]]\n",
        "  print()\n",
        "  print(labels)\n",
        "  preds = result.split()\n",
        "  print()\n",
        "  print(preds)\n",
        "\n",
        "  weights = (0.4, 0.3, 0.2, 0.1)\n",
        "  smoothing_function=SmoothingFunction().method1\n",
        "  bleu = sentence_bleu(labels, preds, weights = weights, smoothing_function = smoothing_function)\n",
        "\n",
        "  return bleu"
      ],
      "metadata": {
        "id": "auM6QQEu1PC5"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate_bleu(6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5WnNhebCdRu",
        "outputId": "7a9cb73a-f909-40d0-94c1-013357329824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[[\"b'one\", 'man', 'lays', 'down', 'on', 'a', 'lawn', 'table', 'in', 'the', 'background', 'while', 'an', 'older', 'lady', 'stands', ',', 'looking', 'down', 'in', 'the', 'foreground', 'a', 'man', 'is', 'passed', 'out', 'asleep', 'on', 'a', 'patio', 'table', 'while', 'his', 'mother', 'looks', 'down', 'at', 'the', 'ground'], ['a', 'man', 'in', 'black', 'is', 'sitting', 'at', 'a', 'table', 'taking', 'a', 'photo', 'of', 'an', 'older', 'woman', 'on', 'a', 'patio'], ['an', 'older', 'lady', 'walking', 'by', 'a', 'man', 'who', 'is', 'laying', 'on', 'the', 'table', 'looking', 'at', 'something'], ['a', 'younger', 'man', 'takes', 'a', 'picture', 'of', 'what', 'an', 'older', 'women', 'stands', 'over']]\n",
            "\n",
            "['a', 'man', 'in', 'a', 'black', 'shirt', 'is', 'standing', 'in', 'front', 'of', 'a', 'building']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.19696803427432394"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_bleu(batch_size):\n",
        "  labels_list = []\n",
        "  preds_list = []\n",
        "  for images, labels in test_raw_filtered.batch(batch_size).take(1):\n",
        "    break\n",
        "\n",
        "  for image in images:\n",
        "    result = model.simple_gen(image, temperature=0.0)\n",
        "    preds = result.split()\n",
        "    preds_list.append(preds)\n",
        "\n",
        "  for label in labels:    \n",
        "    label = label.numpy().tolist()                                    # Cast tensor to list\n",
        "    label = [item[:-1].decode().lower().split() for item in label]    # Split sentence into list of words + remove . from end\n",
        "    labels_list.append(label)\n",
        "\n",
        "  weights = (0.4, 0.3, 0.2, 0.1)\n",
        "  smoothing_function=SmoothingFunction().method1\n",
        "  bleu = corpus_bleu(labels_list, preds_list, weights = weights, smoothing_function = smoothing_function)\n",
        "  # print(bleu)\n",
        "\n",
        "  return bleu"
      ],
      "metadata": {
        "id": "ieSmTpO5QvYm"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOhjHqgv3F2e"
      },
      "source": [
        "### Callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dyQN9UfJYEd"
      },
      "source": [
        "For feedback during training setup a `keras.callbacks.Callback` to generate some captions for the surfer image at the end of each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "IKDwbZOCZ-AP"
      },
      "outputs": [],
      "source": [
        "class GenerateText(tf.keras.callbacks.Callback):\n",
        "  def __init__(self):\n",
        "    image_url = 'https://tensorflow.org/images/surf.jpg'\n",
        "    image_path = tf.keras.utils.get_file('surf.jpg', origin=image_url)\n",
        "    self.image = load_image(image_path)\n",
        "\n",
        "  def on_epoch_end(self, epochs=None, logs=None):\n",
        "    print()\n",
        "    print()\n",
        "    for t in (0.0, 0.5, 1.0):\n",
        "      result = self.model.simple_gen(self.image, False,temperature=t)\n",
        "      print(result)\n",
        "    # beam_res = self.model.simple_gen(self.image, beam_size=3)\n",
        "    # print(result)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yNA3_RAsdl0"
      },
      "source": [
        "It generates three output strings, like the earlier example, like before the first is \"greedy\", choosing the argmax of the logits at each step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGVLpzo13rcA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "444f934a-818a-4384-ce8c-4e3e62b42408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "on surrounded in its sharp hill sleeping sitting on to another past carrying soldiers a woman\n",
            "\n"
          ]
        }
      ],
      "source": [
        "g = GenerateText()\n",
        "g.model = model\n",
        "g.on_epoch_end(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAxp4KZRKDk9"
      },
      "source": [
        "Also use `callbacks.EarlyStopping` to terminate training when the model starts to overfit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "MjzrwGZp23xx"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    GenerateText(),\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        patience=5, restore_best_weights=True\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBaJhQpcG8u0"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBXG0dCDKO55"
      },
      "source": [
        "Configure and execute the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "2OR5ZpAII__u"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss=masked_loss,    # Model goal is to minimize the loss\n",
        "    metrics=[masked_acc] \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro955bQ2KR0X"
      },
      "source": [
        "For more frequent reporting, use the `Dataset.repeat()` method, and set the `steps_per_epoch` and `validation_steps` arguments to `Model.fit`. \n",
        "\n",
        "With this setup on `Flickr8k` a full pass over the dataset is 900+ batches, but below the reporting-epochs are 100 steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "3aB0baOVMZe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "746e30a3-51a7-4251-eb0d-7d1f989288c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 5.2720 - masked_acc: 0.1681\n",
            "\n",
            "a man in a man in a man in a man\n",
            "a man in a man on a man in a man in a blue\n",
            "a man in the washing in brown speak in a red\n",
            "\n",
            "100/100 [==============================] - 100s 505ms/step - loss: 5.2720 - masked_acc: 0.1681 - val_loss: 4.9213 - val_masked_acc: 0.2221\n",
            "Epoch 2/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.8497 - masked_acc: 0.2307\n",
            "\n",
            "a man in a man in a white and a white and a white and a white and a white and a white and a man is is is is is is is is is is is is is is is is is is is in a white and\n",
            "a man in a man in a white and a blue blue child in a man is man on a ball\n",
            "a 2 a snowboarder wearing a have men is catch surrounded up the rocks into the stunt\n",
            "\n",
            "100/100 [==============================] - 48s 478ms/step - loss: 4.8497 - masked_acc: 0.2307 - val_loss: 4.6523 - val_masked_acc: 0.2508\n",
            "Epoch 3/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.6575 - masked_acc: 0.2503\n",
            "\n",
            "a man in a blue shirt is in a blue shirt is in a blue\n",
            "a man in a a water\n",
            "two children in the\n",
            "\n",
            "100/100 [==============================] - 37s 368ms/step - loss: 4.6575 - masked_acc: 0.2503 - val_loss: 4.4918 - val_masked_acc: 0.2616\n",
            "Epoch 4/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.5011 - masked_acc: 0.2635\n",
            "\n",
            "a man in a red shirt is in a red and a red and a red and a red and a red and a red and a red and a red and a red and a red and a red and a red and a red and a red\n",
            "a man in a red is in a dog is sitting\n",
            "two a man are walking in a big shoulder\n",
            "\n",
            "100/100 [==============================] - 46s 464ms/step - loss: 4.5011 - masked_acc: 0.2635 - val_loss: 4.3950 - val_masked_acc: 0.2706\n",
            "Epoch 5/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.4219 - masked_acc: 0.2690\n",
            "\n",
            "a man in a blue shirt is is is in a blue and a blue and a blue and a blue and a blue and a blue shirt is is is is is is is is in a blue and a blue and a blue and a blue and\n",
            "a man is is walking in the of a pool\n",
            "two steps one children on a small yellow of paper\n",
            "\n",
            "100/100 [==============================] - 43s 435ms/step - loss: 4.4219 - masked_acc: 0.2690 - val_loss: 4.2772 - val_masked_acc: 0.2817\n",
            "Epoch 6/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.3248 - masked_acc: 0.2787\n",
            "\n",
            "a man in a red shirt is is in a blue shirt is in a blue shirt and a blue shirt and a blue shirt is in the water\n",
            "a woman wearing a black shirt and a red shirt is sitting in the water\n",
            "two standing in big marches hard just of the three people on white water\n",
            "\n",
            "100/100 [==============================] - 42s 422ms/step - loss: 4.3248 - masked_acc: 0.2787 - val_loss: 4.1767 - val_masked_acc: 0.2843\n",
            "Epoch 7/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.2321 - masked_acc: 0.2843\n",
            "\n",
            "a man in a blue shirt is in a blue shirt is in a blue shirt is in a blue and a blue shirt is in a blue and a blue shirt is is in a blue and a blue and a blue and white and white and white\n",
            "a man in a red and a blue shirt is sitting in a white setting\n",
            "an workers dance bears behind his water snow of ocean\n",
            "\n",
            "100/100 [==============================] - 44s 447ms/step - loss: 4.2321 - masked_acc: 0.2843 - val_loss: 4.1093 - val_masked_acc: 0.2979\n",
            "Epoch 8/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.1879 - masked_acc: 0.2866\n",
            "\n",
            "a man in a blue shirt is in a blue shirt is in a blue and a blue shirt is in a blue and a blue shirt is in a blue and a blue and a blue shirt is in a blue and a blue and white shirt is\n",
            "a man in a blue blue and a black shirt is playing in the beach\n",
            "a boy is doing a water s guitar with another sleeveless in a ocean\n",
            "\n",
            "100/100 [==============================] - 47s 475ms/step - loss: 4.1879 - masked_acc: 0.2866 - val_loss: 4.0708 - val_masked_acc: 0.3014\n",
            "Epoch 9/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.1499 - masked_acc: 0.2898\n",
            "\n",
            "a man in a blue shirt is playing in the water\n",
            "a man is playing in the water\n",
            "a child looks on a day on the wave\n",
            "\n",
            "100/100 [==============================] - 35s 347ms/step - loss: 4.1499 - masked_acc: 0.2898 - val_loss: 3.9267 - val_masked_acc: 0.3156\n",
            "Epoch 10/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.0758 - masked_acc: 0.2977\n",
            "\n",
            "a man in a blue shirt is in a blue shirt is in a blue and a blue shirt is in a blue and a blue shirt is in a blue and white and white and white and white shirt is in the water\n",
            "a woman in a white shirt is in a red and white and a red shirt and white shirt is in a pool\n",
            "two pool in a cow slide in front of lane up face\n",
            "\n",
            "100/100 [==============================] - 45s 455ms/step - loss: 4.0758 - masked_acc: 0.2977 - val_loss: 3.8999 - val_masked_acc: 0.3097\n",
            "Epoch 11/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.0531 - masked_acc: 0.2999\n",
            "\n",
            "a man in a red shirt is jumping in the water\n",
            "a girl in a black shirt is playing a edge of a water\n",
            "a woman in a white blue hats next to statue along him\n",
            "\n",
            "100/100 [==============================] - 38s 381ms/step - loss: 4.0531 - masked_acc: 0.2999 - val_loss: 3.9352 - val_masked_acc: 0.3070\n",
            "Epoch 12/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.9826 - masked_acc: 0.3029\n",
            "\n",
            "a man in a red shirt is in a blue shirt is in a blue shirt is in a blue shirt is in a blue shirt is in a blue shirt is in a blue shirt is in a blue shirt is in a blue shirt is in a\n",
            "a woman is playing on a red pool\n",
            "man pulls their pool in a front of a blue comes\n",
            "\n",
            "100/100 [==============================] - 45s 451ms/step - loss: 3.9826 - masked_acc: 0.3029 - val_loss: 3.9156 - val_masked_acc: 0.3064\n",
            "Epoch 13/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.9466 - masked_acc: 0.3113\n",
            "\n",
            "a man in a red shirt is in a blue shirt is in a blue shirt is in a pool\n",
            "a girl in a red shirt is walking in a ocean\n",
            "an man wearing white females kicking the hand on a water to rice\n",
            "\n",
            "100/100 [==============================] - 39s 386ms/step - loss: 3.9466 - masked_acc: 0.3113 - val_loss: 3.8542 - val_masked_acc: 0.3081\n",
            "Epoch 14/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.9100 - masked_acc: 0.3120\n",
            "\n",
            "a man in a red shirt is jumping in the water\n",
            "a child in a black shirt is is playing in a wave\n",
            "a ball soccer woman with a mountains talking through a mountain\n",
            "\n",
            "100/100 [==============================] - 36s 358ms/step - loss: 3.9100 - masked_acc: 0.3120 - val_loss: 3.7803 - val_masked_acc: 0.3180\n",
            "Epoch 15/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.8779 - masked_acc: 0.3135\n",
            "\n",
            "a man in a red shirt is in a blue shirt is jumping in the water\n",
            "a child is in a yellow and blue and yellow shirt is jumping through a pool\n",
            "a young girl is taking a softball trick and glasses one man is standing in one is in the air\n",
            "\n",
            "100/100 [==============================] - 41s 407ms/step - loss: 3.8779 - masked_acc: 0.3135 - val_loss: 3.7754 - val_masked_acc: 0.3182\n",
            "Epoch 16/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.8449 - masked_acc: 0.3160\n",
            "\n",
            "a man in a red shirt is in a blue shirt and white shirt is in a pool\n",
            "a man in a white shirt and a blue jacket is is jumping through the ocean\n",
            "a thumbs man is wave in water\n",
            "\n",
            "100/100 [==============================] - 39s 393ms/step - loss: 3.8449 - masked_acc: 0.3160 - val_loss: 3.7879 - val_masked_acc: 0.3229\n",
            "Epoch 17/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.8477 - masked_acc: 0.3177\n",
            "\n",
            "a man in a red shirt is jumping in the water\n",
            "a man in a white shirt is jumping in the ocean\n",
            "a chef is swimming into the ocean in front of toy and talking on his wave\n",
            "\n",
            "100/100 [==============================] - 36s 359ms/step - loss: 3.8477 - masked_acc: 0.3177 - val_loss: 3.8261 - val_masked_acc: 0.3177\n",
            "Epoch 18/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.8115 - masked_acc: 0.3152\n",
            "\n",
            "a man in a red shirt is in a pool\n",
            "a man is in a blue shirt is playing a pool\n",
            "a woman peacock shirtless wave in the surfboard behind a black swim\n",
            "\n",
            "100/100 [==============================] - 36s 358ms/step - loss: 3.8115 - masked_acc: 0.3152 - val_loss: 3.7746 - val_masked_acc: 0.3132\n",
            "Epoch 19/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.7309 - masked_acc: 0.3212\n",
            "\n",
            "a man in a red shirt is jumping in the ocean\n",
            "a snowboarder in the water is sitting in the water\n",
            "a lady is swimming through a tool\n",
            "\n",
            "100/100 [==============================] - 33s 335ms/step - loss: 3.7309 - masked_acc: 0.3212 - val_loss: 3.6758 - val_masked_acc: 0.3238\n",
            "Epoch 20/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.6833 - masked_acc: 0.3255\n",
            "\n",
            "a man in a red shirt is in a pool\n",
            "a man is swimming in the water\n",
            "a little boy gets a leaping from water to catch jump arts\n",
            "\n",
            "100/100 [==============================] - 33s 334ms/step - loss: 3.6833 - masked_acc: 0.3255 - val_loss: 3.7154 - val_masked_acc: 0.3131\n",
            "Epoch 21/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.6763 - masked_acc: 0.3299\n",
            "\n",
            "a man in a red shirt is swimming in the ocean\n",
            "a boy in a red jacket is in a pool\n",
            "a blondhaired snowboarder chewing a pool drivers hand\n",
            "\n",
            "100/100 [==============================] - 34s 341ms/step - loss: 3.6763 - masked_acc: 0.3299 - val_loss: 3.5935 - val_masked_acc: 0.3362\n",
            "Epoch 22/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.6802 - masked_acc: 0.3253\n",
            "\n",
            "a man is in a blue shirt is in the water\n",
            "a boy is in the water\n",
            "a man in a red striped kayak over a surfboard\n",
            "\n",
            "100/100 [==============================] - 33s 335ms/step - loss: 3.6802 - masked_acc: 0.3253 - val_loss: 3.5956 - val_masked_acc: 0.3334\n",
            "Epoch 23/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.6333 - masked_acc: 0.3244\n",
            "\n",
            "a man in a red shirt is in a blue shirt is in a pool\n",
            "a woman wearing a red hat is swimming on a surfboard\n",
            "a boy in a person in a red pants in a white hat with a bike\n",
            "\n",
            "100/100 [==============================] - 38s 379ms/step - loss: 3.6333 - masked_acc: 0.3244 - val_loss: 3.5962 - val_masked_acc: 0.3302\n",
            "Epoch 24/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.6682 - masked_acc: 0.3269\n",
            "\n",
            "a man in a red shirt is swimming in a pool\n",
            "a man in a red shirt with a red shirt in a red shirt is playing a wave in the water\n",
            "a man raises sailing officer\n",
            "\n",
            "100/100 [==============================] - 38s 379ms/step - loss: 3.6682 - masked_acc: 0.3269 - val_loss: 3.5308 - val_masked_acc: 0.3400\n",
            "Epoch 25/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.6316 - masked_acc: 0.3287\n",
            "\n",
            "a man in a red shirt is swimming in a pool\n",
            "a person in a red hat is leaning in the water\n",
            "a white boat in red swimming swims through the ocean\n",
            "\n",
            "100/100 [==============================] - 36s 358ms/step - loss: 3.6316 - masked_acc: 0.3287 - val_loss: 3.5785 - val_masked_acc: 0.3361\n",
            "Epoch 26/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.5927 - masked_acc: 0.3283\n",
            "\n",
            "a man in a red shirt is in a pool\n",
            "a man is sitting in a pool\n",
            "a couple is riding down the ocean while playing in a surfer is on the water\n",
            "\n",
            "100/100 [==============================] - 38s 376ms/step - loss: 3.5927 - masked_acc: 0.3283 - val_loss: 3.5224 - val_masked_acc: 0.3387\n",
            "Epoch 27/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.6193 - masked_acc: 0.3293\n",
            "\n",
            "a man in a red shirt is in a pool\n",
            "a man is sitting on the ocean in the water\n",
            "a toddler in a swimming over a searching wave holding a wave on a net toy\n",
            "\n",
            "100/100 [==============================] - 35s 352ms/step - loss: 3.6193 - masked_acc: 0.3293 - val_loss: 3.5693 - val_masked_acc: 0.3259\n",
            "Epoch 28/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.5702 - masked_acc: 0.3318\n",
            "\n",
            "a man in a red shirt is swimming in the water\n",
            "a man in a red hat is swimming in the water\n",
            "a surfer with a blue shirt and knees jumps on a sky while in his mouth\n",
            "\n",
            "100/100 [==============================] - 37s 368ms/step - loss: 3.5702 - masked_acc: 0.3318 - val_loss: 3.5540 - val_masked_acc: 0.3373\n",
            "Epoch 29/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.5427 - masked_acc: 0.3364\n",
            "\n",
            "a man in a red shirt is swimming in the water\n",
            "a boy in a wetsuit and a white swimming wave\n",
            "a person performing a swim boat\n",
            "\n",
            "100/100 [==============================] - 34s 336ms/step - loss: 3.5427 - masked_acc: 0.3364 - val_loss: 3.5377 - val_masked_acc: 0.3374\n",
            "Epoch 30/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.5387 - masked_acc: 0.3339\n",
            "\n",
            "a man in a red shirt is in a pool\n",
            "a boy in a red hat is playing in the ocean\n",
            "a man wearing black and blue skirt riding a boat beside the ocean in a coast wheel pin\n",
            "\n",
            "100/100 [==============================] - 37s 376ms/step - loss: 3.5387 - masked_acc: 0.3339 - val_loss: 3.5209 - val_masked_acc: 0.3319\n",
            "Epoch 31/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.5295 - masked_acc: 0.3370\n",
            "\n",
            "a man in a red shirt is swimming in the ocean\n",
            "a man in a red shirt is riding a wave\n",
            "a guy in a hat is on the sand\n",
            "\n",
            "100/100 [==============================] - 36s 364ms/step - loss: 3.5295 - masked_acc: 0.3370 - val_loss: 3.4562 - val_masked_acc: 0.3459\n",
            "Epoch 32/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.4993 - masked_acc: 0.3364\n",
            "\n",
            "a man in a red shirt is swimming in a pool\n",
            "a man in a red shirt is in a water\n",
            "a skier wave through a ocean chalk\n",
            "\n",
            "100/100 [==============================] - 36s 356ms/step - loss: 3.4993 - masked_acc: 0.3364 - val_loss: 3.4705 - val_masked_acc: 0.3434\n",
            "Epoch 33/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.4838 - masked_acc: 0.3416\n",
            "\n",
            "a man in a red shirt is swimming in a pool\n",
            "a man is swims in a wave\n",
            "a man in british onto a surfboard in a boy in the ocean herding a in a big cow\n",
            "\n",
            "100/100 [==============================] - 37s 366ms/step - loss: 3.4838 - masked_acc: 0.3416 - val_loss: 3.4685 - val_masked_acc: 0.3394\n",
            "Epoch 34/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.5028 - masked_acc: 0.3340\n",
            "\n",
            "a man in a red shirt is surfing in a wave\n",
            "a man in a red jacket is jumping a wave\n",
            "a man is about to many log\n",
            "\n",
            "100/100 [==============================] - 35s 349ms/step - loss: 3.5028 - masked_acc: 0.3340 - val_loss: 3.4224 - val_masked_acc: 0.3451\n",
            "Epoch 35/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.4603 - masked_acc: 0.3397\n",
            "\n",
            "a man in a red shirt is surfing in the ocean\n",
            "a person in a red jacket and a white blue shirt is in a wave\n",
            "a man in red jacket and jeans is in the air\n",
            "\n",
            "100/100 [==============================] - 35s 353ms/step - loss: 3.4603 - masked_acc: 0.3397 - val_loss: 3.4007 - val_masked_acc: 0.3458\n",
            "Epoch 36/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.4297 - masked_acc: 0.3427\n",
            "\n",
            "a man in a red shirt is surfing in the ocean\n",
            "a young boy in a red wetsuit is on a wave\n",
            "a toddler in an old red hat runs in the water\n",
            "\n",
            "100/100 [==============================] - 35s 354ms/step - loss: 3.4297 - masked_acc: 0.3427 - val_loss: 3.3834 - val_masked_acc: 0.3468\n",
            "Epoch 37/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.3983 - masked_acc: 0.3427\n",
            "\n",
            "a surfer in a red and yellow hat is swimming in the water\n",
            "a surfer in a red shirt is swimming in the water\n",
            "a person wearing a hat a red shirt and white pants shows a wave into the water of a big blue surfboard\n",
            "\n",
            "100/100 [==============================] - 39s 390ms/step - loss: 3.3983 - masked_acc: 0.3427 - val_loss: 3.4293 - val_masked_acc: 0.3427\n",
            "Epoch 38/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.3841 - masked_acc: 0.3458\n",
            "\n",
            "a man in a red shirt is in a pool\n",
            "a man in a blue shirt is using a wave in a pool\n",
            "a group of boys in water holding a time in the background\n",
            "\n",
            "100/100 [==============================] - 37s 367ms/step - loss: 3.3841 - masked_acc: 0.3458 - val_loss: 3.4181 - val_masked_acc: 0.3402\n",
            "Epoch 39/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.3647 - masked_acc: 0.3493\n",
            "\n",
            "a surfer in a red shirt is surfing in a wave\n",
            "a surfer in a white and white shirt is surfboard on a wave\n",
            "a surfer in a hat goggles stands in the hand of a looks as diver is suspended into a surfer\n",
            "\n",
            "100/100 [==============================] - 38s 383ms/step - loss: 3.3647 - masked_acc: 0.3493 - val_loss: 3.3571 - val_masked_acc: 0.3501\n",
            "Epoch 40/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.3763 - masked_acc: 0.3468\n",
            "\n",
            "a man in a red shirt is surfing in a wave\n",
            "a man in a red and a red shirt is being surfing into the ocean\n",
            "a surfer in a black surfboard on a desert\n",
            "\n",
            "100/100 [==============================] - 37s 373ms/step - loss: 3.3763 - masked_acc: 0.3468 - val_loss: 3.3632 - val_masked_acc: 0.3513\n",
            "Epoch 41/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.3510 - masked_acc: 0.3490\n",
            "\n",
            "a man in a red shirt is swimming in a pool\n",
            "a man is performing a wave in a blue swimsuit\n",
            "a man is splashing down a snowy river\n",
            "\n",
            "100/100 [==============================] - 34s 339ms/step - loss: 3.3510 - masked_acc: 0.3490 - val_loss: 3.3411 - val_masked_acc: 0.3573\n",
            "Epoch 42/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.3233 - masked_acc: 0.3494\n",
            "\n",
            "a man in a red shirt is surfing in a wave\n",
            "a man in a red hat is doing a wave in the ocean\n",
            "a man in a black costume is in a rapids\n",
            "\n",
            "100/100 [==============================] - 34s 345ms/step - loss: 3.3233 - masked_acc: 0.3494 - val_loss: 3.3675 - val_masked_acc: 0.3500\n",
            "Epoch 43/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.3078 - masked_acc: 0.3528\n",
            "\n",
            "a man in a red shirt is surfing in the ocean\n",
            "a surfer in a red shirt is playing in a pool\n",
            "a man is water in the water climbing swims behind a public wave\n",
            "\n",
            "100/100 [==============================] - 36s 357ms/step - loss: 3.3078 - masked_acc: 0.3528 - val_loss: 3.3333 - val_masked_acc: 0.3436\n",
            "Epoch 44/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.3329 - masked_acc: 0.3485\n",
            "\n",
            "a man in a red shirt is surfing in the ocean\n",
            "a man is swimming in the water\n",
            "a person is swimming in the air on a woman head of the ocean\n",
            "\n",
            "100/100 [==============================] - 36s 360ms/step - loss: 3.3329 - masked_acc: 0.3485 - val_loss: 3.3730 - val_masked_acc: 0.3437\n",
            "Epoch 45/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.3563 - masked_acc: 0.3449\n",
            "\n",
            "a man in a red shirt is in a pool\n",
            "a man in a red helmet is surfing a wave in a wave\n",
            "a man in a red wave 5 or falling into a raft flying with water in the arm\n",
            "\n",
            "100/100 [==============================] - 38s 386ms/step - loss: 3.3563 - masked_acc: 0.3449 - val_loss: 3.3221 - val_masked_acc: 0.3514\n",
            "Epoch 46/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.3050 - masked_acc: 0.3498\n",
            "\n",
            "a surfer in a blue shirt is swimming in a wave\n",
            "a surfer in a blue surfboard holding a wave\n",
            "a man in a girl catches a wave\n",
            "\n",
            "100/100 [==============================] - 37s 370ms/step - loss: 3.3050 - masked_acc: 0.3498 - val_loss: 3.3303 - val_masked_acc: 0.3490\n",
            "Epoch 47/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.3163 - masked_acc: 0.3516\n",
            "\n",
            "a man in a red shirt is surfing in a wave\n",
            "a man in a red shirt is riding a wave\n",
            "a man with a bikini is playing in skis\n",
            "\n",
            "100/100 [==============================] - 37s 366ms/step - loss: 3.3163 - masked_acc: 0.3516 - val_loss: 3.3463 - val_masked_acc: 0.3497\n",
            "Epoch 48/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.3105 - masked_acc: 0.3507\n",
            "\n",
            "a man in a red shirt is riding a wave\n",
            "a surfer is in a blue shirt\n",
            "a man is surfing on a kayaking while riding a wave\n",
            "\n",
            "100/100 [==============================] - 37s 370ms/step - loss: 3.3105 - masked_acc: 0.3507 - val_loss: 3.2393 - val_masked_acc: 0.3614\n",
            "Epoch 49/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.2857 - masked_acc: 0.3544\n",
            "\n",
            "a man in a red shirt is surfing in a pool\n",
            "surfer in a red wetsuit is being in a pool\n",
            "a man in a white wetsuit is diving\n",
            "\n",
            "100/100 [==============================] - 34s 338ms/step - loss: 3.2857 - masked_acc: 0.3544 - val_loss: 3.3573 - val_masked_acc: 0.3440\n",
            "Epoch 50/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.2664 - masked_acc: 0.3572\n",
            "\n",
            "a man in a red wetsuit is in a pool\n",
            "a man in a red wetsuit is in a pool\n",
            "a a snowboarder in a red orange cap is throwing a wave\n",
            "\n",
            "100/100 [==============================] - 34s 342ms/step - loss: 3.2664 - masked_acc: 0.3572 - val_loss: 3.3544 - val_masked_acc: 0.3478\n",
            "Epoch 51/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.2555 - masked_acc: 0.3567\n",
            "\n",
            "a man in a red shirt is surfing in a wave\n",
            "a man wearing a red shirt is in a wetsuit in a pool\n",
            "the waterskiing swimmer in the waves in their pool\n",
            "\n",
            "100/100 [==============================] - 35s 352ms/step - loss: 3.2555 - masked_acc: 0.3567 - val_loss: 3.2401 - val_masked_acc: 0.3545\n",
            "Epoch 52/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.2285 - masked_acc: 0.3625\n",
            "\n",
            "a man in a red wetsuit is surfing in a wave\n",
            "a man is surfing in the water\n",
            "a this surfer rides a through the portrait of wave while he is coming towards a wave\n",
            "\n",
            "100/100 [==============================] - 37s 371ms/step - loss: 3.2285 - masked_acc: 0.3625 - val_loss: 3.3138 - val_masked_acc: 0.3529\n",
            "Epoch 53/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.2473 - masked_acc: 0.3584\n",
            "\n",
            "a surfer in a red wetsuit is surfing in the ocean\n",
            "a man in a white jacket is riding a wave\n",
            "a boy riding a surfboard in his bmx pool in a windsurfing\n",
            "\n",
            "100/100 [==============================] - 38s 377ms/step - loss: 3.2473 - masked_acc: 0.3584 - val_loss: 3.2848 - val_masked_acc: 0.3570\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_ds.repeat(),\n",
        "    steps_per_epoch=100,\n",
        "    validation_data=test_ds.repeat(),\n",
        "    validation_steps=20,\n",
        "    epochs=100,\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P634LfVgw-eV"
      },
      "source": [
        "Plot the loss and accuracy over the training run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "6Wn8KSkUw916",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "2cdde3ec-47b0-4f3b-e709-44393a977e38"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f15f823cf70>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('CE/token')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "yZQ78b2Kxw-T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "765cb5c2-d1bd-4e65-eb51-9a8442e93340"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f15f823b610>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(history.history['masked_acc'], label='accuracy')\n",
        "plt.plot(history.history['val_masked_acc'], label='val_accuracy')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('CE/token')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save & Load Model"
      ],
      "metadata": {
        "id": "zdtQpEz21XVM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate model with Test Dataset (corpus bleu metric)"
      ],
      "metadata": {
        "id": "jAgBbAInemBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/save_and_load.ipynb\n",
        "# Save Captioner model\n",
        "model.save('saved_model') "
      ],
      "metadata": {
        "id": "dST2nk0L1gLD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a9254bd-ed22-42c2-bec8-8132e650fb5c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.preprocessing.string_lookup.StringLookup object at 0x7f164050c970>, because it is not built.\n",
            "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 413). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Captioner model\n",
        "test_model = tf.keras.models.load_model('saved_model')\n",
        "# Check its architecture\n",
        "test_model.summary()"
      ],
      "metadata": {
        "id": "9cKOy-1A2FaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### !Bleu score for 100 images"
      ],
      "metadata": {
        "id": "YMLZWC71_3qA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "bleu_score = batch_bleu(100)\n",
        "print(\"Corpus BLEU Score for 100 images: \", bleu_score)\n",
        "print(\"Execution time(s) :\",  time.time() - start_time)"
      ],
      "metadata": {
        "id": "jvapo7PpAAE6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fb0e705-5bd6-4ddf-8da9-521b4b11d1ad"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus BLEU Score for 100 images:  0.26848491134788727\n",
            "Execution time(s) : 693.240877866745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### !Bleu score for 1000 images"
      ],
      "metadata": {
        "id": "-EFvF6AutzqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_score = batch_bleu(1000)\n",
        "print(\"Corpus BLEU Score for 1000 images: \", bleu_score)"
      ],
      "metadata": {
        "id": "2zpdM8GiRC9s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4853a0f4-1916-4ec6-bbcd-2bda33c07dc4"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus BLEU Score for 1000 images:  0.26307498313067007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Parameters chosen (table)"
      ],
      "metadata": {
        "id": "X3J9MSNrtkvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    Optimization    | Parameter          Justification\n",
        "    ----------------------------------------------\n",
        "    Encoder         | mobilenet          Produced better results over the xception encoder (also was a bit faster)\n",
        "    ---\n",
        "    Embeddings      | glove_wiki_300     Pretrained embeddings produced slightly better bleu score than our own generated embeddings (also the training process was faster)\n",
        "    ---\n",
        "    Beam Search     | beam size = 3      Smaller beam size gave less accurate results, while bigger sizes gave same accuracy but required more training time\n",
        "    ---\n",
        "    Decoder         | dropout rate = 0.5\n",
        "    Hyperparameters | units = 300        (οσο το μεγεθος των embeddings)\n",
        "                    | num_layers=8       More layers lead to better accuracy. However, the bigger the layers size the more time consuming is the model tranining. 8 decoder layers is a sweet spot for our training.\n",
        "                    | num_heads=10"
      ],
      "metadata": {
        "id": "LSJXyfjvuGpT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare dataset for mini-competition"
      ],
      "metadata": {
        "id": "LlTBS7FdFBzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPETITION SET FILE (path)\n",
        "competition_files_path = tf.keras.utils.get_file('competition_files.csv',\n",
        "                                          cache_subdir=os.path.abspath('.'),\n",
        "                                          origin='/competition_files.csv',\n",
        "                                          extract=False)"
      ],
      "metadata": {
        "id": "jpw31Xt2FYnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# READ COMPETITION FILES\n",
        "competition_files = (path/competition_files_path).read_text().splitlines()\n",
        "competition_files = [str(path/IMAGE_DIR/fname) for fname in competition_files]\n",
        "# CREATE DATASETS\n",
        "competition_raw = tf.data.experimental.from_list(competition_files)\n",
        "competition_raw.element_spec # Type of Dataset elements"
      ],
      "metadata": {
        "id": "CVPGaUfyHCIJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8299b333-ada1-4535-db2c-47c6190ba301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorSpec(shape=(), dtype=tf.string, name=None)"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD IMAGES\n",
        "competition_ds = (competition_raw\n",
        "        .map(lambda path: (load_image(path)))             # Resize images\n",
        "        .apply(tf.data.experimental.ignore_errors())      # Ignore missing images\n",
        ")"
      ],
      "metadata": {
        "id": "n2N_wyODJ2v2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions\n",
        "competition_predictions = []              # List with predictions for all images\n",
        "for image in competition_ds.take(1):\n",
        "  output = model.simple_gen(image)        # Caption prediction for 1 photo\n",
        "  output = output.split()\n",
        "  competition_predictions.append(output)\n",
        "\n",
        "print(competition_predictions)"
      ],
      "metadata": {
        "id": "gLnDv3e-OHZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to JSON\n",
        "jsonString = json.dumps(competition_predictions)\n",
        "jsonFile = open(\"test_hypotheses.json\", \"w\")\n",
        "jsonFile.write(jsonString)\n",
        "jsonFile.close()"
      ],
      "metadata": {
        "id": "H_RJiU8WP9JF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQN1qT7KNqbL"
      },
      "source": [
        "## Attention plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9XJaC2b2J23"
      },
      "source": [
        "Now, using the trained model,  run that `simple_gen` method on the image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UQPtNTb2eu3"
      },
      "outputs": [],
      "source": [
        "# Surfer image\n",
        "result = model.simple_gen(image, temperature=0.0)\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NXbmeLGN1bJ"
      },
      "source": [
        "Split the output back into tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHKOpm0w5Xto"
      },
      "outputs": [],
      "source": [
        "str_tokens = result.split()\n",
        "str_tokens.append('[END]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE-AjuAV55Qo"
      },
      "source": [
        "The `DecoderLayers` each cache the attention scores for their `CrossAttention` layer. The shape of each attention map is `(batch=1, heads, sequence, image)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZpyuQvq2q-B"
      },
      "outputs": [],
      "source": [
        "attn_maps = [layer.last_attention_scores for layer in model.decoder_layers]\n",
        "[map.shape for map in attn_maps]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T42ImsWv6oHG"
      },
      "source": [
        "So stack the maps along the `batch` axis, then average over the `(batch, heads)` axes, while splitting the `image` axis back into `height, width`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojwtvnkh6mS-"
      },
      "outputs": [],
      "source": [
        "attention_maps = tf.concat(attn_maps, axis=0)\n",
        "attention_maps = einops.reduce(\n",
        "    attention_maps,\n",
        "    'batch heads sequence (height width) -> sequence height width',\n",
        "    height=7, width=7,\n",
        "    reduction='mean'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TM7rA3zGpJW"
      },
      "source": [
        "Now you have a single attention map, for each sequence prediction. The values in each map should sum to `1.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASWmWerGCZp3"
      },
      "outputs": [],
      "source": [
        "einops.reduce(attention_maps, 'sequence height width -> sequence', reduction='sum')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv7XYGFUd-U7"
      },
      "source": [
        "So here is where the model was focusing attention while generating each token of the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fD_y7PD6RPGt"
      },
      "outputs": [],
      "source": [
        "def plot_attention_maps(image, str_tokens, attention_map):\n",
        "    fig = plt.figure(figsize=(16, 9))\n",
        "\n",
        "    len_result = len(str_tokens)\n",
        "    \n",
        "    titles = []\n",
        "    for i in range(len_result):\n",
        "      map = attention_map[i]\n",
        "      grid_size = max(int(np.ceil(len_result/2)), 2)\n",
        "      ax = fig.add_subplot(3, grid_size, i+1)\n",
        "      titles.append(ax.set_title(str_tokens[i]))\n",
        "      img = ax.imshow(image)\n",
        "      ax.imshow(map, cmap='gray', alpha=0.6, extent=img.get_extent(),\n",
        "                clim=[0.0, np.max(map)])\n",
        "\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PI4NAAws9rvY"
      },
      "outputs": [],
      "source": [
        "plot_attention_maps(image/255, str_tokens, attention_maps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riTz0abQKMkV"
      },
      "source": [
        "Now put that together into a more usable function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mktpfW-SKQIJ"
      },
      "outputs": [],
      "source": [
        "@Captioner.add_method\n",
        "def run_and_show_attention(self, image, temperature=0.0):\n",
        "  result_txt = self.simple_gen(image, temperature)\n",
        "  str_tokens = result_txt.split()\n",
        "  str_tokens.append('[END]')\n",
        "\n",
        "  attention_maps = [layer.last_attention_scores for layer in self.decoder_layers]\n",
        "  attention_maps = tf.concat(attention_maps, axis=0)\n",
        "  attention_maps = einops.reduce(\n",
        "      attention_maps,\n",
        "      'batch heads sequence (height width) -> sequence height width',\n",
        "      height=7, width=7,\n",
        "      reduction='mean')\n",
        "  \n",
        "  plot_attention_maps(image/255, str_tokens, attention_maps)\n",
        "  t = plt.suptitle(result_txt)\n",
        "  t.set_y(1.05)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FntRkY11OiMw"
      },
      "outputs": [],
      "source": [
        "run_and_show_attention(model, image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rprk3HEvZuxb"
      },
      "source": [
        "### Try it on your own images\n",
        "\n",
        "For fun, below you're provided a method you can use to caption your own images with the model you've just trained. Keep in mind, it was trained on a relatively small amount of data, and your images may be different from the training data (so be prepared for strange results!)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Psd1quzaAWg"
      },
      "outputs": [],
      "source": [
        "image_url = 'https://tensorflow.org/images/bedroom_hrnet_tutorial.jpg'\n",
        "image_path = tf.keras.utils.get_file(origin=image_url)\n",
        "image = load_image(image_path)\n",
        "\n",
        "run_and_show_attention(model, image)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rBAagBw5p-TM"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}